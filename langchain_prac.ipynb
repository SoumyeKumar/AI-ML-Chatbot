{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import asyncio\n",
    "from pinecone import Pinecone\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "os.environ[\"MISTRAL_API_KEY\"] = os.getenv(\"MISTRAL_AI_API\")\n",
    "\n",
    "from langchain_mistralai import ChatMistralAI\n",
    "\n",
    "model = ChatMistralAI(model=\"mistral-large-latest\")\n",
    "\n",
    "pinecone_api_key = os.environ.get(\"PINECONE_API_KEY\")\n",
    "\n",
    "pc = Pinecone(api_key=pinecone_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from langchain_mistralai import MistralAIEmbeddings\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "# Initialize embedding model\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "from pinecone import ServerlessSpec\n",
    "\n",
    "index_name = \"l6v2\"  # change if desired\n",
    "\n",
    "vector_store= PineconeVectorStore(index_name, embedding=embedding_model.aembed_query)\n",
    "\n",
    "retriever = vector_store.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langgraph.graph import START, MessagesState, StateGraph\n",
    "from langchain_core.messages import HumanMessage,AIMessage\n",
    "from langchain_core.messages import SystemMessage, trim_messages\n",
    "from typing import Sequence\n",
    "from langchain_core.messages import BaseMessage\n",
    "from langgraph.graph.message import add_messages\n",
    "from typing_extensions import Annotated, TypedDict\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a professional Gym/Health Trainer . Answer all questions to the best of your ability in {language} using the following context: {context}, Don't Answer anything unrelated to fitness/health\",\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[Sequence[BaseMessage], add_messages]\n",
    "    language: str\n",
    "\n",
    "\n",
    "trimmer = trim_messages(\n",
    "    max_tokens=65,\n",
    "    strategy=\"last\",\n",
    "    token_counter=model,\n",
    "    include_system=True,\n",
    "    allow_partial=False,\n",
    "    start_on=\"human\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "index = pc.Index(index_name)\n",
    "\n",
    "# Assuming `retriever` is set up as shown in previous answers with Pinecone\n",
    "async def get_context_from_pinecone(query: str, top_k: int = 5) -> str:\n",
    "    \"\"\"Retrieve relevant context from Pinecone based on the query.\"\"\"\n",
    "    # Generate query embedding\n",
    "    query_embedding = embedding_model.embed_query(query)\n",
    "\n",
    "    # Query Pinecone for relevant documents\n",
    "    response = index.query(\n",
    "        vector=query_embedding,\n",
    "        top_k=top_k,\n",
    "        include_metadata=True,\n",
    "        namespace=\"fitness-chatbot\"\n",
    "    )\n",
    "\n",
    "    # Extract context from retrieved documents\n",
    "    context = \" \".join(\n",
    "        [match['metadata']['output'] for match in response['matches']]\n",
    "    ) if response['matches'] else \"No relevant context found.\"\n",
    "\n",
    "    return context\n",
    "\n",
    "# Define the graph\n",
    "workflow = StateGraph(state_schema=State)\n",
    "\n",
    "async def call_model(state: State):\n",
    "    # Define the chain with your prompt and language model\n",
    "    chain = prompt | model\n",
    "\n",
    "    # Extract the user's latest message content\n",
    "    query = state[\"messages\"][-1].content\n",
    "\n",
    "    # Retrieve context from the fitness dataset using RAG\n",
    "    try:\n",
    "        context = await get_context_from_pinecone(query)\n",
    "    except Exception as e:\n",
    "        print(f\"Error retrieving documents: {e}\")\n",
    "        context = \"An error occurred while retrieving relevant context.\"\n",
    "\n",
    "    # Trim and prepare messages with context for the model\n",
    "    trimmed_messages = trimmer.invoke(state[\"messages\"])\n",
    "\n",
    "    # Generate response using context from retrieved docs\n",
    "    try:\n",
    "        response = await chain.ainvoke(\n",
    "            {\n",
    "                \"messages\": trimmed_messages,  # User's message history\n",
    "                \"language\": state[\"language\"],  # User's language preference\n",
    "                \"context\": context              # Context from fitness dataset\n",
    "            }\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating response: {e}\")\n",
    "        return {\"messages\": [HumanMessage(content=\"An error occurred while generating the response.\")]}\n",
    "\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "# Workflow Details\n",
    "workflow.add_edge(START, \"model\")\n",
    "workflow.add_node(\"model\", call_model)\n",
    "\n",
    "memory = MemorySaver()\n",
    "\n",
    "app = workflow.compile(checkpointer=memory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|Hello| V|k|ay|!| How| can| I| assist| you| today| with| your| fitness| or| health| goals|?||"
     ]
    }
   ],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"abc1\"}}\n",
    "query = \"Hi! I'm Vkay.\"\n",
    "language = \"English\"\n",
    "\n",
    "input_messages = [HumanMessage(query)]\n",
    "\n",
    "async for chunk, metadata in app.astream(\n",
    "    {\"messages\": input_messages, \"language\": language},\n",
    "    config,\n",
    "    stream_mode=\"messages\",\n",
    "):\n",
    "    if isinstance(chunk, AIMessage):  # Filter to just model responses\n",
    "        print(chunk.content, end=\"|\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Great to hear that you're headed to the gym! Here's a simple and effective chest workout for you:\n",
      "\n",
      "**Warm-up:**\n",
      "\n",
      "* 5-10 minutes of light cardio (treadmill, stationary bike, or rowing machine)\n",
      "* Dynamic stretches (arm circles, cat-cow stretch, and shoulder rolls)\n",
      "\n",
      "**Workout:**\n",
      "\n",
      "1. **Barbell Bench Press:**\n",
      "\t* 4 sets of 8-12 reps\n",
      "\t* Rest for 60-90 seconds between sets\n",
      "2. **Incline Dumbbell Press:**\n",
      "\t* 3 sets of 10-12 reps\n",
      "\t* Rest for 60 seconds between sets\n",
      "3. **Cable Flyes:**\n",
      "\t* 3 sets of 12-15 reps\n",
      "\t* Rest for 60 seconds between sets\n",
      "4. **Push-ups:**\n",
      "\t* 3 sets to failure\n",
      "\t* Rest for 60 seconds between sets\n",
      "5. **Dumbbell Pullovers:**\n",
      "\t* 2 sets of 12-15 reps\n",
      "\t* Rest for 60 seconds between sets\n",
      "\n",
      "**Cool-down:**\n",
      "\n",
      "* Static stretches for the chest, shoulders, and triceps\n",
      "\n",
      "**Tips:**\n",
      "\n",
      "* Start with a weight that challenges you but allows you to complete the suggested repetitions with good form.\n",
      "* Increase the weight or resistance as you get stronger.\n",
      "* Make sure to engage your core and maintain proper form throughout each exercise.\n",
      "* Don't forget to listen to your body and take breaks when needed.\n",
      "\n",
      "Have a great workout! Remember that consistency is key, and be proud of yourself for taking the time to improve your fitness."
     ]
    }
   ],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"abc1\"}}\n",
    "query = \"Hi! I'm on my way to the gym, I will be hitting chest today. Give me a workout schedule.\"\n",
    "language = \"English\"\n",
    "\n",
    "input_messages = [HumanMessage(query)]\n",
    "\n",
    "async for chunk, metadata in app.astream(\n",
    "    {\"messages\": input_messages, \"language\": language},\n",
    "    config,\n",
    "    stream_mode=\"messages\",\n",
    "):\n",
    "    if isinstance(chunk, AIMessage):  # Filter to just model responses\n",
    "        print(chunk.content, end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Of course, I'm ready to assist with your fitness and health questions. Please provide a clear question or topic related to exercise, nutrition, or wellness, and I'll do my best to provide a helpful and informative response in the following format:\n",
      "\n",
      "Output: [Answer to your question]\n",
      "\n",
      "Let's start whenever you're ready."
     ]
    }
   ],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"abc1\"}}\n",
    "query = \"ugygvghcuyuibujbhuybnyunb\"\n",
    "language = \"English\"\n",
    "\n",
    "input_messages = [HumanMessage(query)]\n",
    "\n",
    "async for chunk, metadata in app.astream(\n",
    "    {\"messages\": input_messages, \"language\": language},\n",
    "    config,\n",
    "    stream_mode=\"messages\",\n",
    "):\n",
    "    if isinstance(chunk, AIMessage):  # Filter to just model responses\n",
    "        print(chunk.content, end=\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
